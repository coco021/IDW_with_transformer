{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubZfqX6t8N8V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1715592919537,
     "user_tz": -480,
     "elapsed": 27228,
     "user": {
      "displayName": "coco chen",
      "userId": "12047055562196465015"
     }
    },
    "outputId": "0b66169d-2597-4948-c95d-9cfd45077834"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fln8F-KkOMBC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim\n",
    "from geographiclib.geodesic import Geodesic\n",
    "# idw 需要的库\n",
    "import geopandas as gpd\n",
    "import math\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from shapely.geometry import Polygon, Point\n",
    "import sklearn.metrics as metrics\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gICUgCKBop1p"
   },
   "source": [
    "# 我的需求\n",
    "首先，它需要是一个回归模型，因为我想要预测每个点的超参数值。不仅是已知点，还包括未知点，那么我应该先随机得到一组未知点（使用buffer），然后对每一个点都计算一个超参数。  \n",
    "\n",
    "\n",
    "我们先进行一次插值，然后再进行参数的计算，这样的话，我们就可以得到每一个未知点的value信息，这样的任务就变成了提高插值精度。\n",
    "\n",
    "然后，输入数据是经度、纬度和降水量，这是一个三维的输入，一维的输出。但是，模型的结构还不知道。在这种情况下，可能需要考虑如何将带有空间信息的输入数据转换为适合的格式？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "lruNmfNwNRuJ"
   },
   "source": [
    "## 先计算一次插值，然后再进行参数的计算\n",
    "这里使用的就是普通的IDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lKOpbbpNOMBE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714274624008,
     "user_tz": -480,
     "elapsed": 20440,
     "user": {
      "displayName": "coco chen",
      "userId": "12047055562196465015"
     }
    },
    "outputId": "6dd0550d-b2c8-419a-ea41-1d1bbd025b91"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 数据处理形成正态分布或0-1的样子，我们的降雨数据没有负数\n",
    "def standardize(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    transformed_data = np.empty(data.shape)\n",
    "    for i in range(len(data)):\n",
    "        transformed_data[i] = (data[i] - mean) / std\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "def normalize(data):\n",
    "    max = np.max(data)\n",
    "    min = np.min(data)\n",
    "    transformed_data = np.empty(data.shape)\n",
    "    for i in range(len(data)):\n",
    "        transformed_data[i] = (data[i] - min) / (max - min)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "def caldis(lon1, lat1, lon2, lat2):  # 输入两点经纬度\n",
    "    a = radians(lat1 - lat2)\n",
    "    b = radians(lon1 - lon2)\n",
    "    lat1, lat2 = radians(lat1), radians(lat2)\n",
    "    t = sin(a / 2) ** 2 + cos(lat1) * cos(lat2) * sin(b / 2) ** 2\n",
    "    distance = 2 * asin(sqrt(t)) * 6378.137\n",
    "\n",
    "    # 设置一个阈值，当距离接近于零时，使用一个非零的默认距离值,防止溢出\n",
    "    threshold = 1e-6\n",
    "    if distance < threshold:\n",
    "        distance = threshold\n",
    "\n",
    "    return distance  # 返回两点距离\n",
    "\n",
    "# parameter是一个超参数，用于调整插值的精度，默认为1，这样不影响没插值时的计算\n",
    "# 我们在这里预设获得的参数排列顺序和输入的点排列顺序是一致的，那么实际上我要在之前进行一个对应的排序\n",
    "def idw(lon, lat, value, x, y, parameter = 1):\n",
    "    prediction_result = []\n",
    "    for i in range(len(x)):\n",
    "        distance_list = []\n",
    "        for j in range(len(lon)):\n",
    "            distance = caldis(lon[j], lat[j], x[i], y[i])\n",
    "            distance_list.append(distance)\n",
    "        # 我需要找到这两个点的超参数，然后再进行插值\n",
    "        sqdis = list(1 / np.power(distance_list, 2))\n",
    "\n",
    "        # 每个点的已知值 * 每个点的权重（未知点到每个已知点的距离） * 每个点的超参数，由于都是相同大小，所以直接相乘\n",
    "        z = np.sum(np.array(value) * np.array(sqdis) * np.array(parameter)) / np.sum(sqdis)\n",
    "        prediction_result.append(z)\n",
    "\n",
    "    return prediction_result  # 返回每个点插值组成的列表\n",
    "\n",
    "# 在缓冲区内随机生成点\n",
    "def random_point_within_polygon(polygon):\n",
    "    min_x, min_y, max_x, max_y = polygon\n",
    "    # 创建了一个具有四个角点的多边形\n",
    "    polygon_obj = Polygon([(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)])  # Create a Polygon object\n",
    "    while True:\n",
    "        random_point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n",
    "        if polygon_obj.contains(random_point):\n",
    "            return random_point\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成随机点，然后进行插值，并计算误差"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQCQVAT3NRuK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714274624008,
     "user_tz": -480,
     "elapsed": 5,
     "user": {
      "displayName": "coco chen",
      "userId": "12047055562196465015"
     }
    },
    "outputId": "d42410f6-ce57-4abc-a44d-8af2787eda8d"
   },
   "outputs": [],
   "source": [
    "# 先读取数据（实际上的文件里面有近几十年来每一年的降水量，不过我们并没有用到）\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/geo-master/random_data.csv\", usecols=['lat', 'lon', '2016'])\n",
    "lat, lon, value = data.lat, data.lon, data['2016']\n",
    "value = normalize(value)\n",
    "\n",
    "\n",
    "# 准备存储随机点的经纬度，已知点信息，随机点信息\n",
    "points = gpd.GeoDataFrame()\n",
    "random_points = gpd.GeoDataFrame()\n",
    "prediction_result = []\n",
    "\n",
    "# 读取这个点（point），生成一个buffer，然后在其中生成一个随机点，这些随机点被存在random_points中\n",
    "for i in range(len(lat)):\n",
    "    # 存储点到geodataframe中\n",
    "    points = pd.concat([points, gpd.GeoDataFrame(geometry=[Point(lon[i], lat[i])])], ignore_index=True)\n",
    "\n",
    "    # 创建buffer\n",
    "    circle_buffer = Point(lon[i], lat[i]).buffer(0.5)\n",
    "\n",
    "    # 生成随机点的个数，1表示每一个已知点周围只生成一个随机点\n",
    "    # for j in range(1):\n",
    "    random_point = random_point_within_polygon(circle_buffer.bounds)\n",
    "\n",
    "        # 存储随机点，随机点的经纬度也存储在geodataframe中了\n",
    "    random_points = pd.concat([random_points, gpd.GeoDataFrame(geometry=[random_point])], ignore_index=True)\n",
    "\n",
    "# 输入数据集，每个样本包含经度、纬度和降水量，读入pd的dataframe中\n",
    "# 用idw插值\n",
    "prediction_result = idw(lon.tolist(), lat.tolist(), value, random_points.geometry.x, random_points.geometry.y)\n",
    "#print(prediction_result)\n",
    "\n",
    "\n",
    "# 计算准确性\n",
    "mae = metrics.mean_absolute_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n",
    "mse = metrics.mean_squared_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(value, prediction_result)  # 1 表示完美预测，0 表示模型与简单平均值的效果相同，负值表示模型预测比直接使用平均值还要差。\n",
    "\n",
    "print(\"The accuracy of mae, mse, rmse, r2:\", (mae, mse, rmse, r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "twuJFGR2NRuL"
   },
   "source": [
    "## 实现一个处理空间数据的Transformer\n",
    "1. 读取数据集，然后将**数据集转换为tensor**，然后将数据集分为训练集和验证集，最后使用DataLoader加载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0f5WWCCNRuL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714274624008,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "coco chen",
      "userId": "12047055562196465015"
     }
    },
    "outputId": "ad99c072-4cc6-47bd-890a-3eca8aa74449"
   },
   "outputs": [],
   "source": [
    "# 数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self): # 其实我感觉在这段代码中，不用这些好像也可以\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "# 数据集需要转换为tensor\n",
    "data = np.array(data)\n",
    "data = torch.tensor(data).float()\n",
    "\n",
    "# 添加一个维度作为seq_lenth，这里还没有特别完整的方案\n",
    "data = data.unsqueeze(1)\n",
    "\n",
    "dataset = MyDataset(data)\n",
    "# 这时候的data是[1000， 3]，1000个样本，3个特征\n",
    "\n",
    "# 分数据集为训练，验证\n",
    "train_size = int(len(data) * 0.5)\n",
    "validate_size = int(len(data) * 0.2)\n",
    "test_size = len(data) - validate_size - train_size\n",
    "train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, validate_size, test_size])\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "batch_size = 8 # 这个怎么选？\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)# 那么我输入时候应该是许多个[16, 3]\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "'''\n",
    "data_iter = iter(train_loader)\n",
    "batch = next(data_iter)\n",
    "print(\"Data shape in the batch:\", batch.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo4o15yks-U2"
   },
   "source": [
    "在[文章](https://arxiv.org/abs/2311.15530)中，能使用transformer处理空间数据，它的做法有\n",
    "1. 将原始数据输入模型时，经过两层全连接，作为input embedding；\n",
    "2. 位置嵌入时，我们根据经纬度获取两两距离和角度矩阵，而不是使用sin cos 的绝对位置；\n",
    "3. 使用上面的位置矩阵，和qk相乘，作为注意力；\n",
    "于是我仿照它处理空间信息的方式，创建New_Transformer模块。\n",
    "\n",
    "新：\n",
    "使用KMeans将点数据划分为patch。使用零填充不足的部分，或者用平均值。\n",
    "计算每个patch的中心位置和每个点在patch中的相对位置。用于embedding，并删除initial_embedding。删除pos_matrix(bs, bs, 2).（实际上是把它换成了patches）\n",
    "转换为Torch后输入。\n",
    "那我有一个问题，这个batch_size还怎么确定？\n",
    "在原来的方法中，我记录了batch中的点的距离和角度。同样的，我在这里也需要记录每个patch中的点的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 将data的点划分为多个patch：仅使用经纬度信息进行kmeans，或者使用GNN?\n",
    "n_patches = 10\n",
    "kmeans = KMeans(n_clusters=n_patches)\n",
    "patch_labels = kmeans.fit_predict(data[:, :2])  # ndarray of shape (n_samples,) 包含每一堆的标签\n",
    "\n",
    "# 创建patch并计算位置嵌入，选择所有属于第i个patch的点。选择这个patch的中心点，然后计算每个点相对于中心点的相对位置。\n",
    "patches = []\n",
    "patch_centers = []\n",
    "for i in range(n_patches):\n",
    "    patch = data[patch_labels == i]\n",
    "    patches.append(patch)\n",
    "    patch_center = patch[:, :2].mean(axis=0)\n",
    "    patch_centers.append(patch_center)\n",
    "\n",
    "# 确保每个patch的大小相同（补零或其他处理）\n",
    "max_patch_size = max(len(patch) for patch in patches)\n",
    "padded_patches = []\n",
    "relative_positions = []\n",
    "\n",
    "for patch, center in zip(patches, patch_centers):\n",
    "    padding = np.zeros((max_patch_size - len(patch), data.shape[1]))\n",
    "    padded_patch = np.vstack((patch, padding))\n",
    "    padded_patches.append(padded_patch)\n",
    "    \n",
    "    relative_pos = np.zeros((max_patch_size, 2))\n",
    "    relative_pos[:len(patch), :] = patch[:, :2] - center\n",
    "    relative_positions.append(relative_pos)\n",
    "    \n",
    "    \n",
    "# 转换为tensor\n",
    "patches = torch.tensor(padded_patches, dtype=torch.float32)\n",
    "relative_pos = torch.tensor(relative_pos, dtype=torch.float32)\n",
    "patch_centers = torch.tensor(patch_centers, dtype=torch.float32)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "修改了一下calc_dist_angle_mat，但是之后不一定会用到"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 输出一个距离和角度矩阵\n",
    "def calc_dist_angle_mat(data):\n",
    "\n",
    "    # 输入经纬度\n",
    "    # dist_angle_mat [bs, bs, 2]\n",
    "    # 它遍历每一对点，因此对于1000个点也是比较大的数据,1000*1000次运算\n",
    "    # 这里的data是一个batch中的部分，所以不需要我手动筛选\n",
    "    lat = data.lat\n",
    "    lon = data.lon\n",
    "    lats = data.index_select(-1, torch.tensor([0]))\n",
    "    lons = data.index_select(-1, torch.tensor([1]))\n",
    "\n",
    "    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n",
    "\n",
    "    for i in range(len(lons)):\n",
    "        for j in range(len(lons)):\n",
    "            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n",
    "            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n",
    "            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n",
    "\n",
    "    #print(dist_angle_mat.shape)\n",
    "    # print(dist_angle_mat)\n",
    "\n",
    "    return dist_angle_mat\n",
    "    \n",
    "def pos_mat(data):\n",
    "\n",
    "    # 同样是bs, bs, 2 但是用两个值表示位置\n",
    "    # 但是这里的data是一个batch的话，我需要筛选才能得到每个点的信息，现在两个办法：\n",
    "    # 1.筛选 2.不用bs了，改用patch作为一组\n",
    "    pos_mat = np.zeros((len(data), len(data), 2))\n",
    "    patch_count = \n",
    "    patch\n",
    "    \n",
    "    return pos_mat\n",
    "# 定义一个active函数\n",
    "def gelu(x):\n",
    "   return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "# 2个全连接\n",
    "class TwoLayerFCN(nn.Module):\n",
    "    def __init__(self, feat_dim, n_hidden1, n_hidden2):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.linear_1 = nn.Linear(feat_dim, n_hidden1)\n",
    "        self.linear_2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "\n",
    "    def forward(self, in_vec, non_linear=False):\n",
    "        \"\"\"pos_vec: absolute position vector, n * feat_dim\"\"\"\n",
    "        assert in_vec.shape[-1] == self.feat_dim, f\"in_vec.shape: {in_vec.shape}, feat_dim:{self.feat_dim}\"\n",
    "\n",
    "        if non_linear:\n",
    "            mid_emb = F.relu(self.linear_1(in_vec))\n",
    "        else:\n",
    "            mid_emb = self.linear_1(in_vec)\n",
    "\n",
    "        output = self.linear_2(mid_emb)\n",
    "        return output\n",
    "# qkc相乘，得到attention的输出\n",
    "class DotProductAttention(nn.Module):# qkc相乘，得到attention的输出\n",
    "    ''' attn: sum over element-wise product of three vectors'''\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.temperature = torch.tensor(temperature, dtype=torch.float)\n",
    "        self.temperature =temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, pos_mat, d_k, d_v, n_head, mask=None):\n",
    "        # pos_mat = [bs, bs, 2] 这里输入的qkv = [bs, d_model][8, 16]\n",
    "        batch_size, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        # Transpose for attention dot product: batch_size x n_head x len_q x dv\n",
    "        # Separate different heads: batch_size x len_q x n_head x dv\n",
    "\n",
    "        r_q1 = q.view(batch_size, len_q, n_head, d_k).permute(0, 2, 1, 3) # torch.Size([8, 2, 1, 16])\n",
    "        r_k1 = k.view(batch_size, len_q, n_head, d_k).permute(0, 2, 1, 3)\n",
    "        r_v1 = v.view(batch_size, len_v, n_head, d_v).permute(0, 2, 1, 3) # torch.Size([8, 2, 1, 16])\n",
    "\n",
    "        attn1 = torch.mul(r_q1.unsqueeze(2), r_k1.unsqueeze(3))# attn1: [batch_size, n_head, len_q, len_q, d_k], c: [bs, bs, d_k]\n",
    "\n",
    "        attn = torch.mul(attn1, pos_mat)  # attn: [batch_size, n_head, len_q, len_q, d_k]  torch.Size([8, 2, 8, 8, 16])\n",
    "        attn = torch.sum(attn, -2)  # attn: [bs, n_head, len_q, len_q]  torch.Size([8, 2, 8, 8])\n",
    "\n",
    "        torch.div(attn, self.temperature) # 除以根号dk\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        # attn: [bs, n_head, len_q, len_q] r_v1: [bs, n_head, len_v, d_v]\n",
    "        # torch.Size([8, 2, 8, 8]) torch.Size([8, 2, 1, 16])\n",
    "        # print(attn.shape, r_v1.shape)\n",
    "        output = torch.mul(attn, r_v1)\n",
    "\n",
    "        return output, attn\n",
    "# 多头注意力\n",
    "class RelativeMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, temperature=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        if temperature is None:\n",
    "            temperature = d_k ** 0.5\n",
    "\n",
    "        # 将RelativePosition类的初始化部分复制到这里\n",
    "        self.linear_1 = nn.Linear(2 , d_k)\n",
    "        self.linear_2 = nn.Linear(d_k, d_k)\n",
    "\n",
    "        self.attention = DotProductAttention(temperature=temperature)\n",
    "        # w_qs, w_ks, w_vs: [d_model, n_head * d_k/d_v]\n",
    "        # w_qs表示对q进行线性变换，\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, pos_mat, mask=None):\n",
    "        d_k, d_v, d_model, n_head = self.d_k, self.d_v, self.d_model, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        \n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q)\n",
    "        k = self.w_ks(k)\n",
    "        v = self.w_vs(v)\n",
    "\n",
    "\n",
    "        # 相对位置嵌入\n",
    "        n_element = pos_mat.shape[0]\n",
    "        positions = torch.tensor(pos_mat.reshape(-1), dtype=torch.float)\n",
    "        a_k = self.linear_2(self.linear_1(positions)).view(n_element, n_element, -1) # from [bs, bs, 2] to [bs, bs, d_k]\n",
    "\n",
    "        if mask is not None:  # used to achieve Shielded Attention\n",
    "            mask = mask.unsqueeze(1)  # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, a_k, d_k, d_v, n_head, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: sz_b x len_q x n_head x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: sz_b x len_q x (n_head*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, -1, n_head * d_v)  # torch.Size([8, 8, 2*16])\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "#位置前馈\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "# 模型主要的部分\n",
    "class New_TransFormer(nn.Module):\n",
    "    def __init__(self, d_k, d_v, num_head, num_layer, d_model, d_feature, dropout=0.1, scale_emb=False, temperature=None, num_patches=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_k = d_k\n",
    "\n",
    "        #self.initial_embedding = TwoLayerFCN(d_feature, d_model, d_model)\n",
    "                # Positional embedding\n",
    "        self.patch_pos_embed = nn.Parameter(torch.ones(1, num_patches, d_model)) # 因为patch没有一个基准，所以创建一个参数矩阵，来表示patch的位置？\n",
    "        self.relative_pos_embed = nn.Linear(2, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # encoder\n",
    "        self.MultiheadAttention = nn.ModuleList([RelativeMultiHeadAttention(num_head, d_model, d_k, d_v, temperature, dropout=dropout) for _ in range(num_layer)])\n",
    "        self.feedforward = nn.ModuleList([PositionWiseFeedForward(d_model, d_model, dropout=dropout) for _ in range(num_layer)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "\n",
    "        self.decoder = TwoLayerFCN(d_model, d_model, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data, patches, relative_positions):\n",
    "        B, N, T, _ = patches.shape\n",
    "        # Flatten patches and apply linear projection\n",
    "        patches = patches.view(B * N, T, -1)\n",
    "        patches = self.projection(patches)\n",
    "        \n",
    "        # 两个部分，一个是patch在整体中的位置，一个是点在patch中的位置\n",
    "        patch_pos_embedding = self.patch_pos_embed.expand(B, -1, -1)\n",
    "        relative_pos_embedding = self.relative_pos_embed(relative_positions.view(B * N, T, -1))\n",
    "        output = patches + patch_pos_embedding.view(B * N, 1, -1) + relative_pos_embedding\n",
    "        \n",
    "    \n",
    "    \n",
    "        #output = self.initial_embedding(data) # from [bs, d_features] to [bs, d_model]\n",
    "        # 接下来经过n_layers层的encoder层，一个encoder 包括一个attention和一个feedforward\n",
    "        # 先计算出距离和角度矩阵，等一下要进行位置嵌入和对q,k相乘，作为注意力\n",
    "        pos_mat = normalize(calc_dist_angle_mat(data))  #  [bs, bs, 2]\n",
    "\n",
    "        if self.scale_emb:  # 这个原理不是很懂\n",
    "            output *= self.d_model ** 0.5\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "\n",
    "        #output = output + pos_mat  # 形状不一样\n",
    "\n",
    "        # 重复i次注意力\n",
    "        for i in range(len(self.MultiheadAttention)):\n",
    "            output, _ = self.MultiheadAttention[i](output, output, output, pos_mat, mask=None)\n",
    "            output = self.feedforward[i](output)\n",
    "\n",
    "\n",
    "        # encoder结束\n",
    "        \"\"\"\n",
    "        原始的mask部分\n",
    "                masked_pos = masked_pos[:, :, None].expand(-1, -1, enc_output.size(-1))  # [batch_size, max_pred, d_model]\n",
    "\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked_1 = torch.gather(enc_output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked_2 = self.layer_norm(self.activ2(self.linear(h_masked_1)))\n",
    "        dec_output = self.decoder(h_masked_2)  # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        \"\"\"\n",
    "        transformed_output = self.layer_norm(self.activ2(self.linear(output)))\n",
    "        dec_output = self.decoder(transformed_output).squeeze(1)  # [batch_size, len_q, d_model] to [batch_size, d_model]\n",
    "\n",
    "        return dec_output\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfE26XaqWSrJ"
   },
   "source": [
    "## 开始训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SwwQSrLypEB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1714280241953,
     "user_tz": -480,
     "elapsed": 222345,
     "user": {
      "displayName": "coco chen",
      "userId": "12047055562196465015"
     }
    },
    "outputId": "0af93471-b72f-49dd-e1f1-2c54dfd22e0c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 模型实例化\n",
    "model = New_TransFormer(d_k=16, d_v=16, num_head=2, num_layer=3, d_model=16, d_feature=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.08)\n",
    "criterion = nn.MSELoss()\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader: # 对每个batch\n",
    "\n",
    "        # 1. forward the model\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # 2. MSE loss of predicting masked elements\n",
    "        loss = criterion(outputs, data) # (result[3], data[3])\n",
    "\n",
    "        # 3. backward and optimization only in train\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss\n",
    "        # running_loss += loss.item()\n",
    "        #tot_loss += loss.item()\n",
    "\n",
    "    #if epoch  % 5 == 0:\n",
    "    epoch_loss = loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "44g_AEkyNRuN"
   },
   "source": [
    "## 使用更新后的参数进行插值\n",
    "这里的IDW部分的改进包括：\n",
    "    1. 对数据提前处理\n",
    "    2. 不对每个网格点插值，而是对每个已知点周围的随机点进行插值。具体是生成一个buffer，然后在其中生成一个随机点，然后对这个随机点进行插值。\n",
    "    3. 增加使用之前计算出的超参数值，更好地预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1LaQcnz1Sis"
   },
   "source": [
    "下面从已经训练好的模型中提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCExm02sOMBG"
   },
   "outputs": [],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 定义一个函数来提取编码器的特征并转换为列表\n",
    "\n",
    "features_list = []\n",
    "with torch.no_grad():\n",
    "    for i in data:\n",
    "        # 前向传播，提取特征\n",
    "        features = model.encoder(i)\n",
    "        # 将特征添加到列表中\n",
    "        features_list.append(features)\n",
    "# 将列表中的张量拼接为一个张量\n",
    "features_tensor = torch.cat(features_list, dim=0)\n",
    "# 将特征张量转换为列表\n",
    "features_list = features_tensor.tolist()\n",
    "\n",
    "\n",
    "# 提取特征并转换为列表\n",
    "extracted_features = extract_features(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy6hqB1YNRuN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 再次用idw插值,不过这次加入了参数\n",
    "prediction_result = idw(lon.tolist(), lat.tolist(), value, random_points.geometry.x, random_points.geometry.y, parameter = extracted_features)\n",
    "print(prediction_result)\n",
    "\n",
    "\n",
    "\n",
    "mae = metrics.mean_absolute_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n",
    "mse = metrics.mean_squared_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = metrics.r2_score(value, prediction_result)  # 1 表示完美预测，0 表示模型与简单平均值的效果相同，负值表示模型预测比直接使用平均值还要差。\n",
    "\n",
    "print(\"The accuracy of mae, mse, rmse, r2:\", (mae, mse, rmse, r2))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
