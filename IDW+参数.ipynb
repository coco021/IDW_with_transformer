{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubZfqX6t8N8V","executionInfo":{"status":"ok","timestamp":1715592919537,"user_tz":-480,"elapsed":27228,"user":{"displayName":"coco chen","userId":"12047055562196465015"}},"outputId":"0b66169d-2597-4948-c95d-9cfd45077834"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"fln8F-KkOMBC"},"outputs":[],"source":["import torch\n","import os\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim\n","from geographiclib.geodesic import Geodesic\n","# idw 需要的库\n","import geopandas as gpd\n","import math\n","from math import radians, sin, cos, asin, sqrt\n","from shapely.geometry import Polygon, Point\n","import sklearn.metrics as metrics\n","import random\n"]},{"cell_type":"markdown","metadata":{"id":"gICUgCKBop1p"},"source":["# 我的需求\n","首先，它需要是一个回归模型，因为我想要预测每个点的超参数值。不仅是已知点，还包括未知点，那么我应该先随机得到一组未知点（使用buffer），然后对每一个点都计算一个超参数。  \n","\n","\n","我们先进行一次插值，然后再进行参数的计算，这样的话，我们就可以得到每一个未知点的value信息，这样的任务就变成了提高插值精度。\n","\n","然后，输入数据是经度、纬度和降水量，这是一个三维的输入，一维的输出。但是，模型的结构还不知道。在这种情况下，可能需要考虑如何将带有空间信息的输入数据转换为适合的格式？\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"lruNmfNwNRuJ"},"source":["## 先计算一次插值，然后再进行参数的计算\n","这里使用的就是普通的IDW"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"lKOpbbpNOMBE","executionInfo":{"status":"ok","timestamp":1714274624008,"user_tz":-480,"elapsed":20440,"user":{"displayName":"coco chen","userId":"12047055562196465015"}},"outputId":"6dd0550d-b2c8-419a-ea41-1d1bbd025b91"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 3)\n","[0.2882836546311902, 0.1773592173844154, 0.2916750907177373, 0.08076697197310274, 0.19947760593555822, 0.1198330976888909, 0.03150314868051745, 0.046498465313351826, 0.11354665663300624, 0.3315552553943386, 0.2100093034501443, 0.23033681449500498, 0.752646444428776, 0.3495527088945861, 0.4105000330749919, 0.49509190287050014, 0.10872744786566176, 0.12321761863502424, 0.23880794904440225, 0.06288193624415225, 0.04120327545427742, 0.3613359295314557, 0.1252291116207842, 0.22476933245576888, 0.37425109743402185, 0.6276876864429018, 0.7908282265666696, 0.09543612612877779, 0.07634678371357281, 0.16559207443514232, 0.1857610586369569, 0.08027377663874487, 0.07481677343155559, 0.5730613026934295, 0.04486059173064385, 0.033547356935364654, 0.311041842934461, 0.44282733327858786, 0.4900984999516575, 0.4394716706131745, 0.028340069708425862, 0.31555218288059594, 0.754255118699208, 0.22594839938570885, 0.2283440239868594, 0.2259997951059354, 0.07255219549584346, 0.044877374604845054, 0.5373421061054562, 0.04092749233904187, 0.035086348908468123, 0.6546352689037217, 0.37763225682720036, 0.04579054415754025, 0.45343913481332704, 0.10650332632370352, 0.2583982413726439, 0.18643763140547448, 0.109896762622934, 0.23493547095944275, 0.06074061787088275, 0.16607652474423298, 0.06111951321474814, 0.17187765509152894, 0.38277593634195534, 0.3098373786334345, 0.1206695250817626, 0.0848539704666427, 0.10965989369677706, 0.08012186848338225, 0.0755354527191213, 0.0852145244939816, 0.19223117420164731, 0.16202101742753983, 0.06710026171901629, 0.17235327597947894, 0.14637241060919826, 0.18694966654214407, 0.20352055932835872, 0.0158903984426804, 0.18558356727239764, 0.6520778143731235, 0.09438299850241022, 0.16322183822817918, 0.44032157916738973, 0.08225785470253487, 0.06715550096412601, 0.15571415442380712, 0.07345896955114894, 0.25122828823470805, 0.43560034431941996, 0.24741329672125195, 0.292216877869941, 0.18498609848350395, 0.04113662140708377, 0.06460262946091674, 0.08745909295116548, 0.17693683066978894, 0.5424721066351594, 0.193990913015158, 0.39825959501646657, 0.04624033852437342, 0.09516552726075121, 0.045694601014972726, 0.08378099151779438, 0.11608741500669342, 0.08045636547286163, 0.5071657999741289, 0.3322181285787006, 0.8500918956065321, 0.25145184993015685, 0.10874732183150822, 0.21379278776611205, 0.11895953309585841, 0.3872977514547613, 0.05372440594311783, 0.2821294434088865, 0.07266562034429355, 0.04026567168427257, 0.31277693013086094, 0.7772140838443381, 0.07129270837083394, 0.04168167620024295, 0.18746423848841648, 0.06943591314465301, 0.44665970072267724, 0.27486235432203626, 0.8311532160393594, 0.5672212424515547, 0.1717289099984571, 0.2430461783456898, 0.6465595715720635, 0.23744670402070162, 0.40529443444266067, 0.12331818187456518, 0.16946779963782557, 0.11373032339838415, 0.1113708287656922, 0.22989595746135263, 0.7166738384874519, 0.01652156326783422, 0.04301841114753371, 0.09282870210719694, 0.04813610253870237, 0.5803436250201807, 0.050591433617192455, 0.2019668035557032, 0.11343616631879518, 0.3522969223169014, 0.3772094013299351, 0.46580486276051525, 0.6048032460677498, 0.7665564752334636, 0.27841051457025096, 0.3806285250880688, 0.0667577524571573, 0.09416715715113141, 0.20022451065584143, 0.655424921076003, 0.003187613051707114, 0.08605745292260182, 0.2676257629049252, 0.6652187590958275, 0.03046625939304247, 0.27629255572673755, 0.7268489250125151, 0.21432183774444977, 0.08542513861661859, 0.271632874085206, 0.17621042597574382, 0.09658105410589861, 0.21804095665402454, 0.036947660854414206, 0.40997420775614074, 0.2716625878178266, 0.38726794244624113, 0.10907243124539723, 0.07236614995582932, 0.3713327275028269, 0.2100618946754611, 0.3992073757977779, 0.05108672210999437, 0.37254009740398464, 0.13740927930407673, 0.31715798565040265, 0.3205117235950104, 0.15344628516229591, 0.1996297965382649, 0.8506722480648294, 0.016229972047137404, 0.28008812918692527, 0.11402905595755765, 0.020116910515419587, 0.5357881519122264, 0.11265763675529787, 0.04876591213416626, 0.20290661330498147, 0.1886147781287203, 0.2073370774468827, 0.14914401290466536, 0.19495649049248837, 0.30824106426045894, 0.03937833524138829, 0.06315629715860197, 0.21271345889909896, 0.03399688350813033, 0.5056592391727278, 0.04479423995476005, 0.06603570539990111, 0.3581651061738386, 0.20889222232476135, 0.21630588002341256, 0.6851140184539478, 0.06769631349059158, 0.044537319671550675, 0.04418080478180237, 0.19097775187592064, 0.19076453684307632, 0.08483928151918975, 0.07844233392100795, 0.13246833181430304, 0.7642461099458696, 0.3984574911214469, 0.25600803389227506, 0.19271141133183228, 0.770089110493131, 0.16407041759751032, 0.21650981939604197, 0.12985389011247217, 0.07370371932237867, 0.08041624426564926, 0.06794452150985263, 0.1841495304436171, 0.05697965898445342, 0.02846236116480814, 0.05183380094965163, 0.18016358917084518, 0.2420404118727821, 0.06136268038041978, 0.32746953417640623, 0.32966167266622387, 0.37452737061932145, 0.3502368560201276, 0.10326273512912444, 0.05149704647568765, 0.04051201091324179, 0.19851274044088296, 0.2505484551954698, 0.03216152199245233, 0.20441518893720848, 0.07207745307037411, 0.05151368588589568, 0.6961053017734997, 0.6149970082646731, 0.23726790111121468, 0.2367605943131629, 0.23655258699705853, 0.08293363741359845, 0.04643876140187484, 0.07820552022363532, 0.05769344460093484, 0.5906730311292524, 0.043474941054418036, 0.05182373589116621, 0.2722850421943943, 0.13623701427354115, 0.3778488829506797, 0.20096387237859345, 0.13287051481379472, 0.23255173670225154, 0.7627463755423317, 0.20866236813354683, 0.2502237831759649, 0.1585152380189769, 0.2523260038465376, 0.3504299424473383, 0.2616996807344319, 0.5562838509244357, 0.18933435513967503, 0.2705659592648734, 0.39772481595152254, 0.0972520886451695, 0.23128686232225476, 0.13205213663753063, 0.17607969242781968, 0.5198938161405943, 0.20847778303158968, 0.35625608182805424, 0.19274004390544813, 0.03696361792897523, 0.09602436488807715, 0.20184408154454747, 0.11944924311639384, 0.7454906732711909, 0.5966876692494651, 0.16609588742095624, 0.25612243577739974, 0.03273024511207204, 0.15894351608182056, 0.12793784522239135, 0.0649252852256127, 0.20569473209577266, 0.19796966431764593, 0.01009554269433584, 0.24704292047222887, 0.1472768942637493, 0.06391838835147468, 0.04164824363637629, 0.16608634596522215, 0.06554130817021682, 0.020609815997839363, 0.10538476288987932, 0.08701327501132715, 0.3061541740526504, 0.01251459020337738, 0.19230957597532924, 0.10975756310247231, 0.6073404969000533, 0.2507073558704741, 0.04883189147234842, 0.1855875896126472, 0.3475075325092548, 0.3595726062547712, 0.03655689892203417, 0.5459483534860288, 0.09329168559412908, 0.05963259859304131, 0.24703611047592403, 0.10582011690112855, 0.41242983956546225, 0.41997626371846114, 0.13595995820444232, 0.3873072064456909, 0.08786646811597847, 0.04390332905397382, 0.21818732021312237, 0.12386751389998686, 0.0528151083849211, 0.19465755674456128, 0.1473416349151773, 0.4690570804847912, 0.13697374169574084, 0.024832799574239724, 0.21205653582510323, 0.17811732934218594, 0.09345763922499586, 0.047746444672195854, 0.5658953815318755, 0.0983309998420548, 0.03329427169044177, 0.22136788632678667, 0.02534839378155416, 0.29758059262298536, 0.08110264325577456, 0.5483807547174994, 0.0912860823767598, 0.2511539296822321, 0.2970421760974372, 0.09334672224850503, 0.03223351978012714, 0.032639411849689826, 0.5987003541803403, 0.28599573696835673, 0.7703330516301734, 0.29137609728349023, 0.059059563835275435, 0.21517096585242318, 0.13275451782681383, 0.24896009527657786, 0.5436136413031275, 0.0936338207459161, 0.19159832333361862, 0.350225723000181, 0.22624665014394868, 0.297554316540131, 0.05692049117477862, 0.15796364882909905, 0.1701767000618202, 0.36218578345258373, 0.22552188025992462, 0.6454182239572703, 0.14095724519394975, 0.2572695760207965, 0.17184824483250868, 0.06703499560111122, 0.2111589769289163, 0.2627106672421502, 0.44434813144716023, 0.2339642459509542, 0.14886191832048937, 0.17598024304083593, 0.058929447086090526, 0.29321971803048685, 0.11015300652055116, 0.3637145128532157, 0.18601880935285514, 0.23749409574065702, 0.45864388990485083, 0.52181550615539, 0.173770747336641, 0.13621426177471288, 0.18733915582453445, 0.43535429070917514, 0.23250403243580303, 0.5551118961733918, 0.44366931054142156, 0.08734234545766593, 0.5638814830600278, 0.25757736388179847, 0.041389520208018275, 0.22639889655581485, 0.39820513346087677, 0.0476068635745835, 0.03859264535391989, 0.05101088292669853, 0.6551135997857207, 0.2839709683829057, 0.2932140165318242, 0.15598155180423207, 0.25647918439528683, 0.2083906327392703, 0.12043072531895908, 0.42953057430631636, 0.16796115045506285, 0.14147560350880645, 0.16638509174009847, 0.17527948053957992, 0.1208981324039715, 0.1683898054582401, 0.4643046921269448, 0.35149476083275205, 0.07280006012611744, 0.22353373986999808, 0.3912409510211184, 0.7819636438308478, 0.6464830621346151, 0.050748993162322974, 0.20887377581041786, 0.1073020132979941, 0.21084940301062916, 0.03457114193460241, 0.20212507487689985, 0.16710030957817346, 0.13936944771584617, 0.23038401794978858, 0.06081165206339153, 0.2024482589344555, 0.09789798532410493, 0.13851474880750694, 0.19867152557149234, 0.19126108433514993, 0.1518287779814156, 0.29267323774144993, 0.16910550266984745, 0.44479150221154684, 0.19715543762824692, 0.3738258882456244, 0.10334682919712278, 0.023093028134347045, 0.12463570005724056, 0.1573031446636217, 0.1121366541392393, 0.07697934049104022, 0.21263728733861376, 0.20569453868561613, 0.2359952095752233, 0.4515877290559636, 0.09472529201213618, 0.26369051195646614, 0.19919630244855097, 0.07844399573192758, 0.08539636535637396, 0.026992666702427916, 0.11538643970631383, 0.12505332289105206, 0.1687408138967717, 0.7124306637034354, 0.19624482362958706, 0.06524998894631542, 0.04227342138723259, 0.19174554343648006, 0.42544797372074883, 0.13801934934816465, 0.20650847089453542, 0.17063479788936867, 0.35045804707337413, 0.12559420992034762, 0.8348755184333664, 0.7406984915533465, 0.11071711546131979, 0.1265202639952446, 0.3626104286415405, 0.18458239448268413, 0.21490942963691295, 0.8691355664848804, 0.20151767577451649, 0.18151820558578022, 0.09173006476622358, 0.12269761848423928, 0.08451253840243067, 0.6793050238532635, 0.39894856502511866, 0.10207303678218393, 0.23361983080457763, 0.06032949540050007, 0.1586045064773356, 0.16369506556881896, 0.3968560089554556, 0.027102635143890424, 0.1702228310778994, 0.3724131226140174, 0.041177581046972236, 0.04676411321415324, 0.032539681050652004, 0.35997482220755356, 0.8512893614835569, 0.3957482174253421, 0.15352370883850808, 0.22338283834224615, 0.2365763454753045, 0.14748848161978398, 0.041117872160522005, 0.13024346508084558, 0.20156125433474595, 0.3711432249320677, 0.15136556065244888, 0.09212006745487364, 0.13440699893060695, 0.04824043425784997, 0.6196530058185262, 0.1143528743373838, 0.6640184865606646, 0.04113200634048872, 0.07052474678852672, 0.05716173595941, 0.19121869063805036, 0.3306971070664188, 0.14806166625414818, 0.07156443876792877, 0.13197249632993704, 0.24437956956103132, 0.26476848719757407, 0.5456287413912803, 0.22850835503659506, 0.11135963922961592, 0.11611384569822955, 0.3250883104823524, 0.7395375320759396, 0.24419365776694452, 0.04554311411227956, 0.1934051096197477, 0.150985097730587, 0.0639564553056698, 0.05963553241972178, 0.3335009819505721, 0.10295865022835066, 0.26562614593481426, 0.039309177566530205, 0.049902745103498104, 0.06128654246693214, 0.22639167614088004, 0.0489544454705692, 0.19225490669953305, 0.16158334330246157, 0.15763011870970994, 0.3555207835445294, 0.4426882055878193, 0.38885621461800085, 0.2748988327001037, 0.22413491506813024, 0.2226043395038283, 0.03742544355119249, 0.6478172824302126, 0.42653107492493125, 0.057459147055240925, 0.17728525393010117, 0.007257370348324965, 0.16726760983393468, 0.04226991918951803, 0.12503190819571489, 0.058661852195347, 0.03435551069685287, 0.3365109425715644, 0.7719019762056106, 0.0773011617614988, 0.20350706174899624, 0.6518382654921373, 0.04306847088653589, 0.47310736247625734, 0.15030314180371723, 0.4337624541407878, 0.50271303228366, 0.08177490683054049, 0.5294211826257226, 0.044173532099148, 0.19086651711346722, 0.44503973070513875, 0.14428206414712647, 0.12324859494065131, 0.37353522433715786, 0.1853006564822853, 0.4243468883642444, 0.02430166654287212, 0.12227645416705844, 0.7312723366441379, 0.03690863020932188, 0.3043615455308537, 0.04232927877801819, 0.14453376410305896, 0.1658621451349661, 0.08574221198832425, 0.7034797780687718, 0.19101323055150943, 0.1738523653564546, 0.3711019545597089, 0.0385712902438589, 0.1560738392632052, 0.19808731110713462, 0.1611459624431654, 0.5403066146545458, 0.03836124092799261, 0.2334350533207297, 0.3789809389857956, 0.0388638964712784, 0.2122259258959588, 0.36945767191034473, 0.2268253244349108, 0.17530071976519587, 0.050394267372793164, 0.2377165935974319, 0.7356756699518178, 0.1802420862599685, 0.15149998951409352, 0.23268460753358908, 0.3095463632154599, 0.19274566362736723, 0.020024330484641598, 0.5698497911198767, 0.4015069770226515, 0.5751487185969818, 0.18788016611685357, 0.05951126833256978, 0.028930111467753566, 0.022684168149938514, 0.14948537195297568, 0.22097229270087182, 0.1728206263823978, 0.16174579438292802, 0.27015497854813075, 0.0873446216625106, 0.21582202477146045, 0.23384831426513236, 0.4900824731021468, 0.49458004573666703, 0.5020218542766313, 0.020998938801183937, 0.08600411293221698, 0.05570380671361545, 0.35313064255554083, 0.08655676338873698, 0.19318009865562066, 0.08962738588690602, 0.13444024749228878, 0.28622146176457736, 0.18815982871109613, 0.2700212546338341, 0.1631895253781066, 0.03841590929420263, 0.4161054641024875, 0.042368105492658424, 0.2468059932196424, 0.1432672253716426, 0.5045575754246439, 0.11507684299407854, 0.02124412250734444, 0.3878787949003462, 0.203907017811524, 0.37051129428973084, 0.07161177390302656, 0.0404546845847145, 0.20461844478019792, 0.6312697193753157, 0.8727001647541652, 0.2081282536817704, 0.021389592258373297, 0.17323778086413263, 0.10696045389524315, 0.38304325588855354, 0.2223718891807815, 0.6213093544560895, 0.18949110555138507, 0.09085328718110199, 0.15275778072286747, 0.04792827285795223, 0.36127647311785216, 0.093414136972385, 0.08906693926651815, 0.18098488649148534, 0.034915857609792114, 0.06249437508119067, 0.029052805324676096, 0.050946170367573415, 0.5756309376411934, 0.21555024872722692, 0.23975935804969087, 0.13826709568411974, 0.4025034805070996, 0.06598886065902215, 0.10920074705620658, 0.4246896869146782, 0.15843196291640593, 0.1433267262156617, 0.09910254205198421, 0.2002325777467834, 0.0640615865806521, 0.1547962219867562, 0.1943421284195953, 0.13438305041290258, 0.09740799323445068, 0.047537632972110694, 0.19054314619239146, 0.10047057657896709, 0.03365316020061568, 0.143568645290701, 0.05724949238180095, 0.1630301377695144, 0.10011794899574243, 0.06811021566339594, 0.10102840614189326, 0.09614107793448348, 0.27533311042209274, 0.1933034256864633, 0.1335620038291933, 0.2006374231071271, 0.05286409686267029, 0.04292367562308928, 0.12355108453340945, 0.14162466577711427, 0.34034889308627997, 0.42188455631602606, 0.12866654463013352, 0.7365739664523461, 0.0647334339864171, 0.08481487934872424, 0.4030448327904257, 0.09783153925131931, 0.5779279472140404, 0.08017148017231958, 0.04215636601185137, 0.5341116308731515, 0.06410486547413433, 0.013560447561079714, 0.11046826938377027, 0.43441587892006467, 0.04043406408970527, 0.22411627072582277, 0.14941028015522156, 0.05443448682870265, 0.056488620575972266, 0.05018447296561248, 0.3689406520586043, 0.11837225290612431, 0.20965350316332687, 0.19004332098656537, 0.033242567320931914, 0.24569047813834538, 0.15837854776801835, 0.16521441110864624, 0.6348564482171433, 0.1704397326430471, 0.5054113615360002, 0.13520864690495085, 0.15627933808992542, 0.39228796919667874, 0.23008211666560735, 0.26729772153154485, 0.20129905028345138, 0.3360707187725057, 0.045665790493407465, 0.09386447475334454, 0.020224265360101728, 0.035947383205155704, 0.21135461103529374, 0.19919566016154958, 0.534545833039189, 0.19248205825807493, 0.06667310317258236, 0.34701359479162097, 0.18303938684486107, 0.06807581012738048, 0.829834002114509, 0.2769685291368796, 0.24741933712003344, 0.5325087335147715, 0.8189018124099432, 0.23370376536641718, 0.43141037503422647, 0.21508657442223142, 0.5824126256854893, 0.2127753542051676, 0.20160033201240207, 0.07324018075008398, 0.531778528365295, 0.2375771789651173, 0.18591944758731488, 0.16896941558653028, 0.2411874535122344, 0.095631558244929, 0.09458913152941942, 0.11162801434504889, 0.06305191208043517, 0.0595927821578775, 0.37936853376285445, 0.25112943106462704, 0.05027464291262103, 0.08279777665666248, 0.10232136319095149, 0.16691687934494925, 0.03949064636004907, 0.37993227173246796, 0.053343488301550016, 0.10999559328068441, 0.14272686788451744, 0.7203310391105255, 0.8053094469172931, 0.13734654723954723, 0.02753302961449963, 0.034639154229498564, 0.029897238107116093, 0.043953028126591666, 0.10130548188898082, 0.3282986361957091, 0.047609328567320634, 0.16265264311439628, 0.06338156155496716, 0.15967208893705379, 0.058344038032653193, 0.28457708674305904, 0.1247351267068756, 0.09150258106239394, 0.34254783655638715, 0.7177718313110991, 0.17915772849715714, 0.15763585403035246, 0.05999950917529564, 0.08416668168030704, 0.2835249136047377, 0.026725201764281965, 0.2626808047684903, 0.3524363422686781, 0.032219189498572685, 0.06431518655377513, 0.2046531202292146, 0.21950002577911254, 0.2037165991516309, 0.1560972636636347, 0.41547368650808947, 0.032086591214638124, 0.20277490863232964, 0.19790422707286628, 0.16571251690996397, 0.4017778237746705, 0.3049710155509592, 0.7560868453452938, 0.15372414167056314, 0.40697268374660317, 0.19891117937660638, 0.07454848250799086, 0.26577188734884516, 0.15621071493405858, 0.19344899695982626, 0.11338937417785944, 0.07953512059180796, 0.1909399931123582, 0.20361235281434908, 0.47149358806412434, 0.6753001584500371, 0.20323496615467593, 0.11476877430047641, 0.04008565346982858, 0.26605137539450874, 0.1937289417263868, 0.3855670929515796, 0.0827280779947384, 0.10783652599169516, 0.26548036999764174, 0.0347845904328148, 0.6105138246860967, 0.12047532534411866, 0.05950276834559963, 0.38216975889577337, 0.10016310442827947, 0.07246479993545793, 0.08265358129383209, 0.4135785904508208, 0.6174861044709055, 0.521736012846828, 0.3424460203589583, 0.06011280715740816, 0.05974724858423491, 0.49709828998154937, 0.1978801967642033, 0.3788027912913807, 0.14193779610249813, 0.2686925883779173, 0.19757766273310892, 0.40500796288778196, 0.3264739619373041, 0.1098258431721568, 0.38154810881401763, 0.12630641935622855, 0.4147437209573513, 0.1513765963344687, 0.16775796054041564, 0.22920196410889684, 0.37504182409407855, 0.4963053303117554, 0.183156899062156, 0.16308882190312438, 0.03740609917047989, 0.0683924819889223, 0.03505427664640696, 0.19147841965268433, 0.4206665005839226, 0.2755371274085021, 0.27857133350851404, 0.05292855728485548, 0.042427859497875385, 0.10649655402070533, 0.03881537983477155, 0.30980074676383745, 0.032686277703165074, 0.2277151355938836, 0.7032375501572977, 0.3801681442259885, 0.0713083531249773, 0.6547361593031662, 0.2231882529538722, 0.199382032732359, 0.2565941377162438, 0.4644808325463282, 0.13873111612714645, 0.21007506071958085, 0.3812377674686319, 0.10846062552591881, 0.03744959083695971, 0.360619276086902, 0.22554894811810455, 0.24273303469414118, 0.08804520058514292, 0.16186420239366087, 0.2957211405364367, 0.10378448451826906, 0.7016715748700699, 0.16005814967822748, 0.021773171083596055, 0.13951662247087357, 0.3818368609917526, 0.22956604886184123, 0.16419756573330543, 0.3131738818470618, 0.17479138592847301, 0.22880416239775472, 0.0262619714283962, 0.07908270754998983, 0.21264866002326893, 0.15852058216057213, 0.21751442080010805, 0.2715309505990632, 0.7782905373022935, 0.15582055088701433, 0.24267428034334854, 0.18408680756135784, 0.24548486539944706, 0.6076182693673075, 0.1203607975503423, 0.06822490007350876, 0.29491255171747965, 0.03804564788214947, 0.07615577177383187, 0.08571816678780061, 0.14357861404477865, 0.20091640273170724, 0.18190425000566035, 0.23518031421290725, 0.5822320275101023, 0.05974873166369019, 0.18786520732004264, 0.15938914663365567, 0.35697025064829824, 0.20956835079015193, 0.05388523535661373, 0.4485311622728886, 0.20582446543944363, 0.0109400913926037, 0.43838037871800745, 0.08457196884970304, 0.22164802124097566, 0.06642847053234653, 0.15848472800807373]\n"]}],"source":["\n","# 数据处理形成正态分布或0-1的样子，我们的降雨数据没有负数\n","def standardize(data):\n","    mean = np.mean(data)\n","    std = np.std(data)\n","    transformed_data = np.empty(data.shape)\n","    for i in range(len(data)):\n","        transformed_data[i] = (data[i] - mean) / std\n","\n","    return transformed_data\n","\n","def normalize(data):\n","    max = np.max(data)\n","    min = np.min(data)\n","    transformed_data = np.empty(data.shape)\n","    for i in range(len(data)):\n","        transformed_data[i] = (data[i] - min) / (max - min)\n","\n","    return transformed_data\n","\n","def caldis(lon1, lat1, lon2, lat2):  # 输入两点经纬度\n","    a = radians(lat1 - lat2)\n","    b = radians(lon1 - lon2)\n","    lat1, lat2 = radians(lat1), radians(lat2)\n","    t = sin(a / 2) ** 2 + cos(lat1) * cos(lat2) * sin(b / 2) ** 2\n","    distance = 2 * asin(sqrt(t)) * 6378.137\n","\n","    # 设置一个阈值，当距离接近于零时，使用一个非零的默认距离值,防止溢出\n","    threshold = 1e-6\n","    if distance < threshold:\n","        distance = threshold\n","\n","    return distance  # 返回两点距离\n","\n","# parameter是一个超参数，用于调整插值的精度，默认为1，这样不影响没插值时的计算\n","# 我们在这里预设获得的参数排列顺序和输入的点排列顺序是一致的，那么实际上我要在之前进行一个对应的排序\n","def idw(lon, lat, value, x, y, parameter = 1):\n","    prediction_result = []\n","    for i in range(len(x)):\n","        distance_list = []\n","        for j in range(len(lon)):\n","            distance = caldis(lon[j], lat[j], x[i], y[i])\n","            distance_list.append(distance)\n","        # 我需要找到这两个点的超参数，然后再进行插值\n","        sqdis = list(1 / np.power(distance_list, 2))\n","\n","        # 每个点的已知值 * 每个点的权重（未知点到每个已知点的距离） * 每个点的超参数，由于都是相同大小，所以直接相乘\n","        z = np.sum(np.array(value) * np.array(sqdis) * np.array(parameter)) / np.sum(sqdis)\n","        prediction_result.append(z)\n","\n","    return prediction_result  # 返回每个点插值组成的列表\n","\n","# 在缓冲区内随机生成点\n","def random_point_within_polygon(polygon):\n","    min_x, min_y, max_x, max_y = polygon\n","    # 创建了一个具有四个角点的多边形\n","    polygon_obj = Polygon([(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)])  # Create a Polygon object\n","    while True:\n","        random_point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n","        if polygon_obj.contains(random_point):\n","            return random_point\n","\n","\n","# 先读取数据（实际上的文件里面有近几十年来每一年的降水量，不过我们并没有用到）\n","data = pd.read_csv(\"/content/drive/MyDrive/geo-master/random_data.csv\", usecols=['lat', 'lon', '2016'])\n","lat, lon, value = data.lat, data.lon, data['2016']\n","value = normalize(value)\n","print(data.shape)\n","# 准备存储随机点的经纬度，已知点信息，随机点信息\n","points = gpd.GeoDataFrame()\n","random_points = gpd.GeoDataFrame()\n","prediction_result = []\n","\n","# 读取这个点（point），生成一个buffer，然后在其中生成一个随机点，这些随机点被存在random_points中\n","for i in range(len(lat)):\n","    # 存储点到geodataframe中\n","    points = pd.concat([points, gpd.GeoDataFrame(geometry=[Point(lon[i], lat[i])])], ignore_index=True)\n","\n","    # 创建buffer\n","    circle_buffer = Point(lon[i], lat[i]).buffer(0.5)\n","\n","    # 生成随机点的个数，1表示每一个已知点周围只生成一个随机点\n","    # for j in range(1):\n","    random_point = random_point_within_polygon(circle_buffer.bounds)\n","\n","        # 存储随机点，随机点的经纬度也存储在geodataframe中了\n","    random_points = pd.concat([random_points, gpd.GeoDataFrame(geometry=[random_point])], ignore_index=True)\n","\n","# 输入数据集，每个样本包含经度、纬度和降水量，读入pd的dataframe中\n","# 用idw插值\n","prediction_result = idw(lon.tolist(), lat.tolist(), value, random_points.geometry.x, random_points.geometry.y)\n","print(prediction_result)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQCQVAT3NRuK","executionInfo":{"status":"ok","timestamp":1714274624008,"user_tz":-480,"elapsed":5,"user":{"displayName":"coco chen","userId":"12047055562196465015"}},"outputId":"d42410f6-ce57-4abc-a44d-8af2787eda8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy of mae, mse, rmse, r2: (0.023438011290939687, 0.0012358672518851088, 0.03515490366769775, 0.9718642448067439)\n"]}],"source":["\n","# 计算准确性\n","mae = metrics.mean_absolute_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n","mse = metrics.mean_squared_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n","rmse = np.sqrt(mse)\n","r2 = metrics.r2_score(value, prediction_result)  # 1 表示完美预测，0 表示模型与简单平均值的效果相同，负值表示模型预测比直接使用平均值还要差。\n","\n","print(\"The accuracy of mae, mse, rmse, r2:\", (mae, mse, rmse, r2))\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"twuJFGR2NRuL"},"source":["## 实现一个处理空间数据的Transformer\n","1. 读取数据集，然后将**数据集转换为tensor**，然后将数据集分为训练集和验证集，最后使用DataLoader加载数据集。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0f5WWCCNRuL","executionInfo":{"status":"ok","timestamp":1714274624008,"user_tz":-480,"elapsed":4,"user":{"displayName":"coco chen","userId":"12047055562196465015"}},"outputId":"ad99c072-4cc6-47bd-890a-3eca8aa74449"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1000, 1, 3])\n","Data shape in the batch: torch.Size([8, 1, 3])\n"]}],"source":["# 数据集\n","class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self): # 其实我感觉在这段代码中，不用这些好像也可以\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample\n","\n","\n","# 数据集需要转换为tensor\n","data = np.array(data)\n","data = torch.tensor(data).float()\n","\n","# 添加一个维度作为seq_lenth，这里还没有特别完整的方案\n","data = data.unsqueeze(1)\n","\n","dataset = MyDataset(data)\n","# 这时候的data是[1000， 3]，1000个样本，3个特征\n","\n","# 分数据集为训练，验证\n","train_size = int(len(data) * 0.5)\n","validate_size = int(len(data) * 0.2)\n","test_size = len(data) - validate_size - train_size\n","train_dataset, validate_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, validate_size, test_size])\n","\n","# 使用DataLoader加载数据\n","batch_size = 8 # 这个怎么选？\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)# 那么我输入时候应该是许多个[16, 3]\n","validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","# 为什么inputs可以直接用？我没有定义过它。\n","print(data.shape)\n","# 这里我想查看每个批次的数据结构\n","data_iter = iter(train_loader)\n","batch = next(data_iter)\n","print(\"Data shape in the batch:\", batch.shape)  # 输出批次中数据的形状"]},{"cell_type":"markdown","metadata":{"id":"Uo4o15yks-U2"},"source":["在[文章](https://arxiv.org/abs/2311.15530)中，能使用transformer处理空间数据，它的做法有\n","1. 将原始数据输入模型时，经过两层全连接，作为input embedding；\n","2. 位置嵌入时，我们根据经纬度获取两两距离和角度矩阵，而不是使用sin cos 的绝对位置嵌入；\n","3. 使用上面的位置矩阵，和qk相乘，作为注意力；\n","\n","于是我仿照它处理空间信息的方式，创建New_Transformer模块。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPK0oHFxRTBV"},"outputs":[],"source":["\n","# 输出一个距离和角度矩阵\n","def calc_dist_angle_mat(data, out_path=None):\n","\n","    # 输入经纬度\n","    # dist_angle_mat = 例如[16, 16, 2], [bs, bs, 2]\n","    # 它遍历每一对点，因此对于1000个点也是比较大的数据,1000*1000次运算\n","    # 这里的data是一个batch中的部分，现在需要我拿出-1维度的第0 1行\n","    lats = data.index_select(-1, torch.tensor([0]))\n","    lons = data.index_select(-1, torch.tensor([1]))\n","\n","    dist_angle_mat = np.zeros((len(lons), len(lons), 2))\n","\n","    for i in range(len(lons)):\n","        for j in range(len(lons)):\n","            dist = Geodesic.WGS84.Inverse(lats[i], lons[i], lats[j], lons[j])\n","            dist_angle_mat[i, j, 0] = dist[\"s12\"] / 1000.0  # distance, km\n","            dist_angle_mat[i, j, 1] = dist[\"azi1\"]  # azimuth at the first point in degrees\n","\n","    #print(dist_angle_mat.shape)\n","    # print(dist_angle_mat)\n","    if out_path is not None:\n","        np.save(out_path, dist_angle_mat)\n","    else:\n","        return dist_angle_mat\n","# 定义一个active函数\n","def gelu(x):\n","   return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","# 2个全连接\n","class TwoLayerFCN(nn.Module):\n","    def __init__(self, feat_dim, n_hidden1, n_hidden2):\n","        super().__init__()\n","        self.feat_dim = feat_dim\n","        self.linear_1 = nn.Linear(feat_dim, n_hidden1)\n","        self.linear_2 = nn.Linear(n_hidden1, n_hidden2)\n","\n","    def forward(self, in_vec, non_linear=False):\n","        \"\"\"pos_vec: absolute position vector, n * feat_dim\"\"\"\n","        assert in_vec.shape[-1] == self.feat_dim, f\"in_vec.shape: {in_vec.shape}, feat_dim:{self.feat_dim}\"\n","\n","        if non_linear:\n","            mid_emb = F.relu(self.linear_1(in_vec))\n","        else:\n","            mid_emb = self.linear_1(in_vec)\n","\n","        output = self.linear_2(mid_emb)\n","        return output\n","# qkc相乘，得到attention的输出\n","class DotProductAttention(nn.Module):# qkc相乘，得到attention的输出\n","    ''' attn: sum over element-wise product of three vectors'''\n","    def __init__(self, temperature, attn_dropout=0.1):\n","        super().__init__()\n","\n","        #self.temperature = torch.tensor(temperature, dtype=torch.float)\n","        self.temperature =temperature\n","        self.dropout = nn.Dropout(attn_dropout)\n","\n","    def forward(self, q, k, v, pos_mat, d_k, d_v, n_head, mask=None):\n","        # pos_mat = [bs, bs, 2] 这里输入的qkv = [bs, d_model][8, 16]\n","        batch_size, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n","\n","        # Transpose for attention dot product: batch_size x n_head x len_q x dv\n","        # Separate different heads: batch_size x len_q x n_head x dv\n","\n","        r_q1 = q.view(batch_size, len_q, n_head, d_k).permute(0, 2, 1, 3) # torch.Size([8, 2, 1, 16])\n","        r_k1 = k.view(batch_size, len_q, n_head, d_k).permute(0, 2, 1, 3)\n","        r_v1 = v.view(batch_size, len_v, n_head, d_v).permute(0, 2, 1, 3) # torch.Size([8, 2, 1, 16])\n","\n","        attn1 = torch.mul(r_q1.unsqueeze(2), r_k1.unsqueeze(3))# attn1: [batch_size, n_head, len_q, len_q, d_k], c: [bs, bs, d_k]\n","\n","        attn = torch.mul(attn1, pos_mat)  # attn: [batch_size, n_head, len_q, len_q, d_k]  torch.Size([8, 2, 8, 8, 16])\n","        attn = torch.sum(attn, -2)  # attn: [bs, n_head, len_q, len_q]  torch.Size([8, 2, 8, 8])\n","\n","        torch.div(attn, self.temperature) # 除以根号dk\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(mask == 0, -1e10)\n","\n","        attn = self.dropout(F.softmax(attn, dim=-1))\n","        # attn: [bs, n_head, len_q, len_q] r_v1: [bs, n_head, len_v, d_v]\n","        # torch.Size([8, 2, 8, 8]) torch.Size([8, 2, 1, 16])\n","        # print(attn.shape, r_v1.shape)\n","        output = torch.mul(attn, r_v1)\n","\n","        return output, attn\n","# 多头注意力\n","class RelativeMultiHeadAttention(nn.Module):\n","    def __init__(self, n_head, d_model, d_k, d_v, pos_dim, temperature=None, dropout=0.1):\n","        super().__init__()\n","\n","        self.n_head = n_head\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.d_v = d_v\n","\n","        if temperature is None:\n","            temperature = d_k ** 0.5\n","\n","        # 将RelativePosition类的初始化部分复制到这里\n","        self.linear_1 = nn.Linear(pos_dim, d_k)\n","        self.linear_2 = nn.Linear(d_k, d_k)\n","\n","        self.attention = DotProductAttention(temperature=temperature)\n","        # w_qs, w_ks, w_vs: [d_model, n_head * d_k/d_v]\n","        # w_qs表示对q进行线性变换，\n","        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n","        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","\n","    def forward(self, q, k, v, pos_mat, mask=None):\n","        d_k, d_v, d_model, n_head = self.d_k, self.d_v, self.d_model, self.n_head\n","        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n","\n","        residual = q\n","\n","        # Pass through the pre-attention projection: b x lq x (n*dv)\n","        q = self.w_qs(q)\n","        k = self.w_ks(k)\n","        v = self.w_vs(v)\n","\n","        # generate the spatial relative position embeddings (SRPEs)\n","        # 相对位置嵌入\n","        n_element,pos_dim = pos_mat.shape[0], pos_mat.shape[-1]\n","        positions = torch.tensor(pos_mat.reshape(-1, pos_dim), dtype=torch.float)\n","        a_k = self.linear_2(self.linear_1(positions)).view(n_element, n_element, -1) # from [bs, bs, pos_dim] to [bs, bs, d_k]\n","\n","        if mask is not None:  # used to achieve Shielded Attention\n","            mask = mask.unsqueeze(1)  # For head axis broadcasting.\n","\n","        q, attn = self.attention(q, k, v, a_k, d_k, d_v, n_head, mask=mask)\n","\n","        # Transpose to move the head dimension back: sz_b x len_q x n_head x dv\n","        # Combine the last two dimensions to concatenate all the heads together: sz_b x len_q x (n_head*dv)\n","        q = q.transpose(1, 2).contiguous().view(sz_b, -1, n_head * d_v)  # torch.Size([8, 8, 2*16])\n","        q = self.dropout(self.fc(q))\n","        q += residual\n","\n","        q = self.layer_norm(q)\n","\n","        return q, attn\n","#位置前馈\n","class PositionWiseFeedForward(nn.Module):\n","    ''' A two-feed-forward-layer module '''\n","    def __init__(self, d_in, d_hid, dropout=0.1):\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n","        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n","        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        x = self.w_2(F.relu(self.w_1(x)))\n","        x = self.dropout(x)\n","        x += residual\n","\n","        x = self.layer_norm(x)\n","\n","        return x\n","\n","# 模型主要的部分\n","class New_TransFormer(nn.Module):\n","    def __init__(self, d_k, d_v, pos_dim, num_head, num_layer, d_model, d_feature, dropout=0.1, scale_emb=False, temperature=None):\n","        super().__init__()\n","        \"\"\"\n","\n","        \"\"\"\n","        self.d_model = d_model\n","        self.scale_emb = scale_emb\n","        self.d_k = d_k\n","\n","\n","        self.initial_embedding = TwoLayerFCN(d_feature, d_model, d_model)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","\n","        # encoder层\n","        self.MultiheadAttention = nn.ModuleList([RelativeMultiHeadAttention(num_head, d_model, d_k, d_v, pos_dim, temperature, dropout=dropout) for _ in range(num_layer)])\n","        self.feedforward = nn.ModuleList([PositionWiseFeedForward(d_model, d_model, dropout=dropout) for _ in range(num_layer)])\n","\n","        self.linear = nn.Linear(d_model, d_model)\n","        self.activ2 = gelu\n","\n","        self.decoder = TwoLayerFCN(d_model, d_model, 1)\n","\n","\n","\n","    def forward(self, data):\n","\n","        output = self.initial_embedding(data) # from [bs, d_features] to [bs, d_model]\n","\n","        # 接下来经过n_layers层的encoder层，一个encoder 包括一个attention和一个feedforward\n","        # 先计算出距离和角度矩阵，等一下要进行位置嵌入和对q,k相乘，作为注意力\n","        pos_mat = normalize(calc_dist_angle_mat(data))  #  [bs, bs, 2]\n","\n","        if self.scale_emb:  # 这个原理不是很懂\n","            output *= self.d_model ** 0.5\n","\n","        output = self.dropout(output)\n","        output = self.layer_norm(output)\n","\n","\n","        #output = output + pos_mat  # 形状不一样\n","\n","\n","        for i in range(len(self.MultiheadAttention)):\n","            output, _ = self.MultiheadAttention[i](output, output, output, pos_mat, mask=None)\n","            output = self.feedforward[i](output)\n","\n","\n","        # encoder结束\n","        \"\"\"\n","        原始的mask部分\n","                masked_pos = masked_pos[:, :, None].expand(-1, -1, enc_output.size(-1))  # [batch_size, max_pred, d_model]\n","\n","        # get masked position from final output of transformer.\n","        h_masked_1 = torch.gather(enc_output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n","        h_masked_2 = self.layer_norm(self.activ2(self.linear(h_masked_1)))\n","        dec_output = self.decoder(h_masked_2)  # [batch_size, max_pred, n_vocab]\n","\n","        \"\"\"\n","        transformed_output = self.layer_norm(self.activ2(self.linear(output)))\n","        dec_output = self.decoder(transformed_output).squeeze(1)  # [batch_size, len_q, d_model] to [batch_size, d_model]\n","\n","        return dec_output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SfE26XaqWSrJ"},"source":["## 开始训练\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SwwQSrLypEB","executionInfo":{"status":"ok","timestamp":1714280241953,"user_tz":-480,"elapsed":222345,"user":{"displayName":"coco chen","userId":"12047055562196465015"}},"outputId":"0af93471-b72f-49dd-e1f1-2c54dfd22e0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 266.1187\n","Epoch [2/100], Loss: 79.9165\n","Epoch [3/100], Loss: 338.5593\n","Epoch [4/100], Loss: 372.9377\n","Epoch [5/100], Loss: 96.6220\n","Epoch [6/100], Loss: 880.3873\n","Epoch [7/100], Loss: 343.5558\n","Epoch [8/100], Loss: 381.4203\n","Epoch [9/100], Loss: 115.5352\n","Epoch [10/100], Loss: 271.0070\n","Epoch [11/100], Loss: 324.0550\n","Epoch [12/100], Loss: 84.2669\n","Epoch [13/100], Loss: 489.0641\n","Epoch [14/100], Loss: 87.8876\n","Epoch [15/100], Loss: 966.3408\n","Epoch [16/100], Loss: 195.6602\n","Epoch [17/100], Loss: 518.8478\n","Epoch [18/100], Loss: 550.5182\n","Epoch [19/100], Loss: 247.7840\n","Epoch [20/100], Loss: 187.5889\n","Epoch [21/100], Loss: 796.7185\n","Epoch [22/100], Loss: 590.8317\n","Epoch [23/100], Loss: 473.0712\n","Epoch [24/100], Loss: 146.4992\n","Epoch [25/100], Loss: 291.8953\n","Epoch [26/100], Loss: 1399.4399\n","Epoch [27/100], Loss: 93.7055\n","Epoch [28/100], Loss: 82.0994\n","Epoch [29/100], Loss: 358.7386\n","Epoch [30/100], Loss: 133.8734\n","Epoch [31/100], Loss: 500.0169\n","Epoch [32/100], Loss: 149.9963\n","Epoch [33/100], Loss: 97.0559\n","Epoch [34/100], Loss: 165.7898\n","Epoch [35/100], Loss: 347.8104\n","Epoch [36/100], Loss: 109.5296\n","Epoch [37/100], Loss: 484.1837\n","Epoch [38/100], Loss: 52.0152\n","Epoch [39/100], Loss: 267.5836\n","Epoch [40/100], Loss: 172.1632\n","Epoch [41/100], Loss: 140.8458\n","Epoch [42/100], Loss: 60.8217\n","Epoch [43/100], Loss: 116.1587\n","Epoch [44/100], Loss: 348.9396\n","Epoch [45/100], Loss: 754.5782\n","Epoch [46/100], Loss: 95.2553\n","Epoch [47/100], Loss: 955.3156\n","Epoch [48/100], Loss: 472.7124\n","Epoch [49/100], Loss: 122.6416\n","Epoch [50/100], Loss: 582.6032\n","Epoch [51/100], Loss: 121.2801\n","Epoch [52/100], Loss: 630.3784\n","Epoch [53/100], Loss: 151.6956\n","Epoch [54/100], Loss: 256.6062\n","Epoch [55/100], Loss: 483.8521\n","Epoch [56/100], Loss: 245.7412\n","Epoch [57/100], Loss: 984.7990\n","Epoch [58/100], Loss: 100.0875\n","Epoch [59/100], Loss: 154.3391\n","Epoch [60/100], Loss: 85.1345\n","Epoch [61/100], Loss: 62.3553\n","Epoch [62/100], Loss: 260.6585\n","Epoch [63/100], Loss: 623.5173\n","Epoch [64/100], Loss: 125.0451\n","Epoch [65/100], Loss: 100.6132\n","Epoch [66/100], Loss: 113.5125\n","Epoch [67/100], Loss: 272.7234\n","Epoch [68/100], Loss: 808.6483\n","Epoch [69/100], Loss: 583.3745\n","Epoch [70/100], Loss: 137.0999\n","Epoch [71/100], Loss: 439.9728\n","Epoch [72/100], Loss: 978.4960\n","Epoch [73/100], Loss: 112.4208\n","Epoch [74/100], Loss: 451.1681\n","Epoch [75/100], Loss: 201.5303\n","Epoch [76/100], Loss: 641.3895\n","Epoch [77/100], Loss: 233.8525\n","Epoch [78/100], Loss: 214.0575\n","Epoch [79/100], Loss: 272.6854\n","Epoch [80/100], Loss: 407.8827\n","Epoch [81/100], Loss: 480.3087\n","Epoch [82/100], Loss: 82.5973\n","Epoch [83/100], Loss: 70.9352\n","Epoch [84/100], Loss: 87.1473\n","Epoch [85/100], Loss: 97.4395\n","Epoch [86/100], Loss: 213.6736\n","Epoch [87/100], Loss: 515.2059\n","Epoch [88/100], Loss: 126.7882\n","Epoch [89/100], Loss: 440.1306\n","Epoch [90/100], Loss: 293.6935\n","Epoch [91/100], Loss: 268.9496\n","Epoch [92/100], Loss: 579.8247\n","Epoch [93/100], Loss: 1297.1499\n","Epoch [94/100], Loss: 140.3297\n","Epoch [95/100], Loss: 501.7332\n","Epoch [96/100], Loss: 110.5993\n","Epoch [97/100], Loss: 502.5875\n","Epoch [98/100], Loss: 269.7891\n","Epoch [99/100], Loss: 378.4767\n","Epoch [100/100], Loss: 468.8752\n"]}],"source":["\n","# 模型实例化\n","model = New_TransFormer(d_k=16, d_v=16, pos_dim=2, num_head=2, num_layer=3, d_model=16, d_feature=3)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.08)\n","criterion = nn.MSELoss()\n","# print(model)\n","\n","\n","# 训练模型\n","num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","    for data in train_loader: # 对每个batch\n","\n","        # 1. forward the model\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","\n","        # 2. MSE loss of predicting masked elements\n","        loss = criterion(outputs, data) # (result[3], data[3])\n","\n","        # 3. backward and optimization only in train\n","        loss.backward()\n","        # optimizer.step()\n","        optimizer.step()\n","\n","        # loss\n","        # running_loss += loss.item()\n","        #tot_loss += loss.item()\n","\n","    #if epoch  % 5 == 0:\n","    epoch_loss = loss / len(train_loader.dataset)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"44g_AEkyNRuN"},"source":["## 使用更新后的参数进行插值\n","这里的IDW部分的改进包括：\n","    1. 对数据提前处理\n","    2. 不对每个网格点插值，而是对每个已知点周围的随机点进行插值。具体是生成一个buffer，然后在其中生成一个随机点，然后对这个随机点进行插值。\n","    3. 增加使用之前计算出的超参数值，更好地预测。"]},{"cell_type":"markdown","metadata":{"id":"j1LaQcnz1Sis"},"source":["下面从已经训练好的模型中提取特征"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCExm02sOMBG"},"outputs":[],"source":["# 设置模型为评估模式\n","model.eval()\n","\n","# 定义一个函数来提取编码器的特征并转换为列表\n","\n","features_list = []\n","with torch.no_grad():\n","    for i in data:\n","        # 前向传播，提取特征\n","        features = model.encoder(i)\n","        # 将特征添加到列表中\n","        features_list.append(features)\n","# 将列表中的张量拼接为一个张量\n","features_tensor = torch.cat(features_list, dim=0)\n","# 将特征张量转换为列表\n","features_list = features_tensor.tolist()\n","\n","\n","# 提取特征并转换为列表\n","extracted_features = extract_features(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yy6hqB1YNRuN"},"outputs":[],"source":["\n","# 再次用idw插值,不过这次加入了参数\n","prediction_result = idw(lon.tolist(), lat.tolist(), value, random_points.geometry.x, random_points.geometry.y, parameter = extracted_features)\n","print(prediction_result)\n","\n","\n","# 计算准确性\n","mae = metrics.mean_absolute_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n","mse = metrics.mean_squared_error(value, prediction_result)  # 0 表示完美预测，值越大表示预测误差越大。\n","rmse = np.sqrt(mse)\n","r2 = metrics.r2_score(value, prediction_result)  # 1 表示完美预测，0 表示模型与简单平均值的效果相同，负值表示模型预测比直接使用平均值还要差。\n","\n","print(\"The accuracy of mae, mse, rmse, r2:\", (mae, mse, rmse, r2))\n"]}],"metadata":{"celltoolbar":"Raw Cell Format","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"}},"nbformat":4,"nbformat_minor":0}